{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System für Bibliona\n",
    "\n",
    "**Team:** Sarah Blatz, Ida Krämer, Annika Rathai, Markus Kühnle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Projekt entwickeln wir ein kollaboratives Empfehlungssystem für die Buchplattform Bibliona. Ziel ist es, personalisierte Buchempfehlungen auf Basis vergangener Nutzerbewertungen zu generieren. Wir setzen dabei auf ein bewährtes SVD-Modell (Singular Value Decomposition), das Nutzer- und Item-Latenzfaktoren lernt und daraus individuelle Vorhersagen ableitet.\n",
    "\n",
    "Zu Beginn analysieren wir die Datenqualität, insbesondere mögliche Cold-Start-Probleme und die Nutzbarkeit von Metadaten für contentbasierte Verfahren. Auf Basis dieser Analyse bereinigen wir anschließend das Rating-Set, indem wir zu dünn besetzte Nutzer- und Item-Profile entfernen, und schaffen so eine stabilere Datengrundlage für das Modelltraining. Daraufhin trainieren wir ein SVD-Modell mit festen Hyperparametern und evaluieren dessen Leistung mit klassischen Fehlermaßen wie MAE und RMSE sowie mit Top-N-Metriken wie Precision@K und Recall@K, um die Qualität der generierten Empfehlungen zu bewerten.\n",
    "\n",
    "Das System generiert Top-N-Empfehlungen inklusive Confidence-Score (High/Medium/Low) und einer einfachen Erklärungskomponente, die transparent aufzeigt, wie sich die finale Vorhersage zusammensetzt (globaler Mittelwert + Biases + Interaktion). Damit entsteht ein robustes, nachvollziehbares Empfehlungssystem, das praxisnah für reale Anwendungsszenarien auf der Plattform ausgelegt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum SVD statt User-Based und Item-Based Collaborative Filtering?\n",
    "\n",
    "Nach der Analyse verschiedener kollaborativer Filtering-Ansätze haben wir uns für **SVD (Singular Value Decomposition)** entschieden, da es gegenüber klassischen User-Based und Item-Based Methoden entscheidende Vorteile bietet. Während User-Based CF ähnliche Nutzer sucht und deren Bewertungen übernimmt, und Item-Based CF Ähnlichkeiten zwischen Items berechnet, löst SVD das fundamentale Problem der **hohen Dimensionalität und Sparsity** unserer Nutzer-Item-Matrix durch **Latent Factor Modeling**. Das Modell lernt automatisch versteckte Faktoren (z.B. Genre-Präferenzen, Lesestil, Komplexitätsgrad), die Nutzer und Items in einem niedrigdimensionalen Vektorraum repräsentieren. Diese **Latenzfaktoren** ermöglichen es, auch bei extrem spärlichen Daten (wie in unserem Fall mit über 98% Leerwerten in den Metadaten) aussagekräftige Ähnlichkeiten zu finden. Zusätzlich bietet SVD durch die explizite Modellierung von **globalen, Nutzer- und Item-Biases** eine bessere Interpretierbarkeit der Vorhersagen und kann systematische Bewertungsunterschiede zwischen Nutzern (streng vs. großzügig) und Items (beliebt vs. unbekannt) berücksichtigen. Die finale Vorhersage ergibt sich dabei aus der Summe von globalem Durchschnitt, individuellen Biases und der Interaktion zwischen den latenten Nutzer- und Item-Vektoren, was eine transparente Erklärung der Empfehlungen ermöglicht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://gitlabci:****@gitlab.sigmalto.com/api/v4/projects/573/packages/pypi/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn==1.7.0 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: scikit-surprise==1.1.4 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (1.1.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (from scikit-learn==1.7.0) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (from scikit-learn==1.7.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/markuskuehnle/Documents/projects/bigdata-recsys/.venv/lib/python3.10/site-packages (from scikit-learn==1.7.0) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4 scikit-learn==1.7.0 scikit-surprise==1.1.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "ratings_df: pd.DataFrame = pd.read_csv('../data/Bewertungsmatrix_Bibliona.csv')\n",
    "itemprofile_df: pd.DataFrame = pd.read_csv('../data/Itemprofile_Bibliona.csv')\n",
    "bewertung_df: pd.DataFrame = pd.read_csv('../data/Itemprofile_Bibliona.csv')\n",
    "test_df: pd.DataFrame = pd.read_csv('../data/Testdaten_Bibliona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_ID</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Publication_Year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Author_(mei) Kan, fei er de</th>\n",
       "      <th>Author_A. A. Milne</th>\n",
       "      <th>Author_A. Manette Ansay</th>\n",
       "      <th>Author_A. S. Byatt</th>\n",
       "      <th>Author_AMY TAN</th>\n",
       "      <th>...</th>\n",
       "      <th>Genre_literary fiction</th>\n",
       "      <th>Genre_open_syllabus_project</th>\n",
       "      <th>Genre_orphans</th>\n",
       "      <th>Genre_psychological fiction</th>\n",
       "      <th>Genre_science fiction</th>\n",
       "      <th>Genre_suicide</th>\n",
       "      <th>Genre_suspense</th>\n",
       "      <th>Genre_suspense fiction</th>\n",
       "      <th>Genre_thrillers</th>\n",
       "      <th>Genre_Écoles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0440234743</td>\n",
       "      <td>The Testament</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Dell</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0452264464</td>\n",
       "      <td>Beloved (Plume Contemporary Fiction)</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>Plume</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0971880107</td>\n",
       "      <td>Wild Animus</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Too Far</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0345402871</td>\n",
       "      <td>Airframe</td>\n",
       "      <td>431.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0345417623</td>\n",
       "      <td>Timeline</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_ID                            Book-Title  Pages  Publication_Year  \\\n",
       "0  0440234743                         The Testament    NaN            2000.0   \n",
       "1  0452264464  Beloved (Plume Contemporary Fiction)  275.0            1988.0   \n",
       "2  0971880107                           Wild Animus  315.0            2004.0   \n",
       "3  0345402871                              Airframe  431.0            1997.0   \n",
       "4  0345417623                              Timeline  496.0            2000.0   \n",
       "\n",
       "          Publisher  Author_(mei) Kan, fei er de  Author_A. A. Milne  \\\n",
       "0              Dell                            0                   0   \n",
       "1             Plume                            0                   0   \n",
       "2           Too Far                            0                   0   \n",
       "3  Ballantine Books                            0                   0   \n",
       "4  Ballantine Books                            0                   0   \n",
       "\n",
       "   Author_A. Manette Ansay  Author_A. S. Byatt  Author_AMY TAN  ...  \\\n",
       "0                        0                   0               0  ...   \n",
       "1                        0                   0               0  ...   \n",
       "2                        0                   0               0  ...   \n",
       "3                        0                   0               0  ...   \n",
       "4                        0                   0               0  ...   \n",
       "\n",
       "   Genre_literary fiction  Genre_open_syllabus_project  Genre_orphans  \\\n",
       "0                       0                            0              0   \n",
       "1                       0                            0              0   \n",
       "2                       0                            0              0   \n",
       "3                       0                            0              0   \n",
       "4                       0                            0              0   \n",
       "\n",
       "   Genre_psychological fiction  Genre_science fiction  Genre_suicide  \\\n",
       "0                            0                      0              0   \n",
       "1                            0                      0              0   \n",
       "2                            0                      0              0   \n",
       "3                            0                      0              0   \n",
       "4                            0                      0              0   \n",
       "\n",
       "   Genre_suspense  Genre_suspense fiction  Genre_thrillers  Genre_Écoles  \n",
       "0               0                       0                0             0  \n",
       "1               0                       0                0             0  \n",
       "2               0                       0                0             0  \n",
       "3               0                       0                0             0  \n",
       "4               0                       0                0             0  \n",
       "\n",
       "[5 rows x 797 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemprofile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Datenqualität besser einzuschätzen, habe ich überprüft, wie viele Bewertungen pro Nutzer vorliegen und wie vollständig die Item-Metadaten sind. Ziel war es zu erkennen, ob Cold-Start-Probleme auftreten könnten und ob sich die Metadaten für contentbasierte Modelle eignen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total users</td>\n",
       "      <td>798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Users with &lt;10 ratings</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Proportion of users with &lt;10 ratings</td>\n",
       "      <td>0.028822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total metadata features</td>\n",
       "      <td>792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metadata sparsity ratio (0 = dense, 1 = empty)</td>\n",
       "      <td>0.982766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Metric       Value\n",
       "0                                     Total users  798.000000\n",
       "1                          Users with <10 ratings   23.000000\n",
       "2            Proportion of users with <10 ratings    0.028822\n",
       "3                         Total metadata features  792.000000\n",
       "4  Metadata sparsity ratio (0 = dense, 1 = empty)    0.982766"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check 1: Distribution of number of books rated per user\n",
    "user_item_rating_counts: pd.Series = ratings_df.drop(columns=['user_ID']).notna().sum(axis=1)\n",
    "\n",
    "# Check 2: Number of users who rated fewer than 10 books\n",
    "number_of_users_with_less_than_10_ratings: int = (user_item_rating_counts < 10).sum()\n",
    "proportion_of_users_with_less_than_10_ratings: float = (user_item_rating_counts < 10).mean()\n",
    "\n",
    "# Check 3: Sparsity of metadata (proportion of non-zero binary features)\n",
    "binary_feature_columns: list[str] = [col for col in itemprofile_df.columns if col.startswith('Genre_') or col.startswith('Author_')]\n",
    "item_binary_feature_matrix: pd.DataFrame = itemprofile_df[binary_feature_columns].fillna(0)\n",
    "metadata_sparsity_ratio: float = (item_binary_feature_matrix == 0).sum().sum() / item_binary_feature_matrix.size\n",
    "\n",
    "# Prepare results for display\n",
    "statistics_summary_dataframe: pd.DataFrame = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total users\",\n",
    "        \"Users with <10 ratings\",\n",
    "        \"Proportion of users with <10 ratings\",\n",
    "        \"Total metadata features\",\n",
    "        \"Metadata sparsity ratio (0 = dense, 1 = empty)\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        len(user_item_rating_counts),\n",
    "        number_of_users_with_less_than_10_ratings,\n",
    "        proportion_of_users_with_less_than_10_ratings,\n",
    "        len(binary_feature_columns),\n",
    "        metadata_sparsity_ratio\n",
    "    ]\n",
    "})\n",
    "\n",
    "statistics_summary_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analyse zeigt: Nur ca. 2,9 % der Nutzer haben weniger als 10 Bewertungen abgegeben, Cold-Start bei Nutzern ist also ein begrenztes Problem. Die Metadaten hingegen sind extrem spärlich (über 98 % Leerwerte), was sie für contentbasierte Ansätze nahezu unbrauchbar macht. Das bestätigt, dass kollaborative Verfahren wie SVD sinnvoller sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unser Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird sichergestellt, dass alle vier bereitgestellten Datensätze geladen werden. Das ist die Grundlage für alles Weitere. Wir holen uns die Bewertungen, das Test-Set, das Item-Profil (Bücher mit Features), und die Bewertungsmatrix für Fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(\n",
    "    ratings_path: str,\n",
    "    test_path: str,\n",
    "    itemprofile_path: str,\n",
    "    bewertungsmatrix_path: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    ratings_df = pd.read_csv(ratings_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    itemprofile_df = pd.read_csv(itemprofile_path)\n",
    "    bewertung_df = pd.read_csv(bewertungsmatrix_path, index_col=0)\n",
    "    return ratings_df, test_df, itemprofile_df, bewertung_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Entfernen von zu dünnen Nutzern und Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir filtern alle Nutzer raus, die zu wenige Bewertungen abgegeben haben, und auch Items, die zu selten bewertet wurden. Das verbessert die Trainingsdatenqualität und reduziert Cold-Start-Probleme im Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sparse_users_items(df: pd.DataFrame, user_thresh: int = 10, item_thresh: int = 5) -> pd.DataFrame:\n",
    "    user_counts = df['user_ID'].value_counts()\n",
    "    item_counts = df['item_ID'].value_counts()\n",
    "    return df[\n",
    "        df['user_ID'].isin(user_counts[user_counts >= user_thresh].index) &\n",
    "        df['item_ID'].isin(item_counts[item_counts >= item_thresh].index)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelltraining mit SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird ein kollaboratives Filtermodell auf Basis von SVD (Singular Value Decomposition) trainiert. Die Hyperparameter wurden bewusst fix gesetzt, da wir ohnehin mit einer limitierten Datenbasis arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_svd_model(ratings_df: pd.DataFrame) -> Tuple[SVD, Dataset]:\n",
    "    reader = Reader(rating_scale=(1, 10))\n",
    "    data = Dataset.load_from_df(ratings_df[['user_ID', 'item_ID', 'rating']], reader)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_factors': [50],\n",
    "        'n_epochs': [30],\n",
    "        'lr_all': [0.005],\n",
    "        'reg_all': [0.02]\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, joblib_verbose=0)\n",
    "    gs.fit(data)\n",
    "\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "    trainset = data.build_full_trainset()\n",
    "    best_algo.fit(trainset)\n",
    "\n",
    "    return best_algo, trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Zusatzfunktionen: Confidence + Erklärbarkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir geben eine einfache Confidence-Stufe für Empfehlungen aus und bieten zusätzlich eine rudimentäre \"Explainability\", d.h. wie sich die Vorhersage zusammensetzt, bestehend aus globalem Durchschnitt, Nutzer- und Item-Bias und Interaktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(score: float) -> str:\n",
    "    if score >= 0.85:\n",
    "        return \"High\"\n",
    "    elif score >= 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "    \n",
    "    \n",
    "def explain_prediction(algo: SVD, user_id: int, item_id: str) -> Dict[str, float]:\n",
    "    details = {}\n",
    "    try:\n",
    "        user_inner = algo.trainset.to_inner_uid(user_id)\n",
    "        item_inner = algo.trainset.to_inner_iid(item_id)\n",
    "        u_bias = algo.pu[user_inner]\n",
    "        i_bias = algo.qi[item_inner]\n",
    "        pred = algo.predict(user_id, item_id)\n",
    "        details = {\n",
    "            \"global_mean\": algo.trainset.global_mean,\n",
    "            \"user_bias\": algo.bu[user_inner],\n",
    "            \"item_bias\": algo.bi[item_inner],\n",
    "            \"interaction\": np.dot(u_bias, i_bias),\n",
    "            \"final_pred\": pred.est\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Empfehlungen generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für einen Nutzer holen wir Top-N Empfehlungen, skalieren die Scores, fügen Confidence und Erklärungen hinzu. Falls Cold Start, nutzen wir die Bewertungsmatrix als Rückfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(\n",
    "    algo: SVD,\n",
    "    trainset,\n",
    "    user_id: int,\n",
    "    top_n: int,\n",
    "    bewertung_df: pd.DataFrame,\n",
    "    itemprofile_df: pd.DataFrame\n",
    ") -> List[Tuple[str, float, str, Dict]]:\n",
    "    try:\n",
    "        inner_user_id = trainset.to_inner_uid(user_id)\n",
    "        rated_items = set(trainset.to_raw_iid(iid) for (iid, _) in trainset.ur[inner_user_id])\n",
    "        all_items = set(trainset._raw2inner_id_items.keys())\n",
    "        unseen_items = list(all_items - rated_items)\n",
    "\n",
    "        raw_predictions = [(iid, algo.predict(user_id, iid).est) for iid in unseen_items]\n",
    "        scores = [s for _, s in raw_predictions]\n",
    "        if scores:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled = scaler.fit_transform(np.array(scores).reshape(-1, 1)).flatten()\n",
    "            enriched_preds = []\n",
    "            for (iid, _), score in zip(raw_predictions, scaled):\n",
    "                confidence = compute_confidence(score)\n",
    "                explanation = explain_prediction(algo, user_id, iid)\n",
    "                enriched_preds.append((iid, score, confidence, explanation))\n",
    "            return sorted(enriched_preds, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return []\n",
    "    except ValueError:\n",
    "        # Cold-start fallback\n",
    "        if str(user_id) in bewertung_df.index:\n",
    "            fallback = list(bewertung_df.loc[str(user_id)].sort_values(ascending=False).head(top_n).items())\n",
    "            return [(iid, 1.0, \"Low\", {}) for iid, _ in fallback]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation mit Precision@K, Recall@K, MAE und RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, ob unser Modell etwas taugt, evaluieren wir es mit Metriken wie MAE, RMSE und natürlich Precision@K & Recall@K (für Top-N-Recommendations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predictions: List[Tuple[str, float, str, Dict]], ground_truth: Set[str], k: int) -> float:\n",
    "    recommended = [item for item, *_ in predictions[:k]]\n",
    "    return len(set(recommended) & ground_truth) / k if k > 0 else 0.0\n",
    "\n",
    "\n",
    "def recall_at_k(predictions: List[Tuple[str, float, str, Dict]], ground_truth: Set[str], k: int) -> float:\n",
    "    recommended = [item for item, *_ in predictions[:k]]\n",
    "    return len(set(recommended) & ground_truth) / len(ground_truth) if ground_truth else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    algo: SVD,\n",
    "    test_df: pd.DataFrame,\n",
    "    trainset,\n",
    "    top_k: int,\n",
    "    bewertung_df: pd.DataFrame,\n",
    "    itemprofile_df: pd.DataFrame,\n",
    "    threshold: int = 7\n",
    ") -> Dict[str, float]:\n",
    "    test_preds = []\n",
    "    precision_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    user_count = 0\n",
    "\n",
    "    for user_id in test_df[\"user_ID\"].unique():\n",
    "        user_test = test_df[test_df[\"user_ID\"] == user_id]\n",
    "        relevant_items = set(user_test[user_test[\"rating\"] >= threshold][\"item_ID\"])\n",
    "        recommendations = get_top_n_recommendations(algo, trainset, user_id, top_k, bewertung_df, itemprofile_df)\n",
    "\n",
    "        if recommendations and relevant_items:\n",
    "            precision_sum += precision_at_k(recommendations, relevant_items, top_k)\n",
    "            recall_sum += recall_at_k(recommendations, relevant_items, top_k)\n",
    "            user_count += 1\n",
    "\n",
    "        for _, row in user_test.iterrows():\n",
    "            test_preds.append(algo.predict(row[\"user_ID\"], row[\"item_ID\"], row[\"rating\"]))\n",
    "\n",
    "    mae = accuracy.mae(test_preds, verbose=False)\n",
    "    rmse = accuracy.rmse(test_preds, verbose=False)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Precision@K\": precision_sum / user_count if user_count else 0.0,\n",
    "        \"Recall@K\": recall_sum / user_count if user_count else 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Funktion zum Ausführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier passiert alles: Daten laden, Vorverarbeitung, Training, Evaluation und Anzeigen der Top-Empfehlungen mit Confidence und Erklärung. Ideal für direkte Runs oder als Einstiegspunkt für weitere Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'MAE': 1.1382262440820807, 'RMSE': 1.5398554806003166, 'Precision@K': 0.003913043478260871, 'Recall@K': 0.02716183574879227}\n",
      "\n",
      "Top 10 recommendations for user 243:\n",
      "Item ID: 0345339738, Score: 1.00, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.343942962588493, 'interaction': 0.06423197052692262, 'final_pred': 9.3844203996359}\n",
      "Item ID: 0877017883, Score: 1.00, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.146294013669592, 'interaction': 0.26046834762399096, 'final_pred': 9.383007827814067}\n",
      "Item ID: 0679723161, Score: 0.97, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.2302347513311311, 'interaction': 0.027248870936890157, 'final_pred': 9.233729088788506}\n",
      "Item ID: 0553274295, Score: 0.96, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.5652208353755457, 'interaction': -0.3570835322008141, 'final_pred': 9.184382769695215}\n",
      "Item ID: 0064400557, Score: 0.95, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.1116968661237203, 'interaction': 0.04250072597974743, 'final_pred': 9.130443058623952}\n",
      "Item ID: 0671041789, Score: 0.94, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 0.8020245821424784, 'interaction': 0.3413853112208284, 'final_pred': 9.11965535988379}\n",
      "Item ID: 0590353403, Score: 0.94, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 0.7519035235367183, 'interaction': 0.37437244369173567, 'final_pred': 9.102521433748937}\n",
      "Item ID: 0439139597, Score: 0.94, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 0.9668713508510541, 'interaction': 0.15851520116600976, 'final_pred': 9.101632018537547}\n",
      "Item ID: 055321313X, Score: 0.93, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 0.9637853646324013, 'interaction': 0.11357745500407798, 'final_pred': 9.053608286156964}\n",
      "Item ID: 0812550706, Score: 0.93, Confidence: High\n",
      "   → Explain: {'global_mean': 7.977304341459844, 'user_bias': -0.0010588749393593089, 'item_bias': 1.079270093501496, 'interaction': -0.02530287467656841, 'final_pred': 9.030212685345413}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ratings_path = '../data/Ratings_Bibliona.csv' # TODO: Pfad zu Ratings angeben\n",
    "    test_path = '../data/Testdaten_Bibliona.csv' # TODO: hier Datensatz zur Evaluation ändern\n",
    "    itemprofile_path = '../data/Itemprofile_Bibliona.csv' # TODO: Pfad zu Itemprofile angeben\n",
    "    bewertung_path = '../data/Bewertungsmatrix_Bibliona.csv' # TODO: Pfad zu Bewertungsmatrix angeben\n",
    "\n",
    "    ratings_df, test_df, itemprofile_df, bewertung_df = load_all_data(\n",
    "        ratings_path, test_path, itemprofile_path, bewertung_path\n",
    "    )\n",
    "\n",
    "    ratings_df = filter_sparse_users_items(ratings_df)\n",
    "    algo, trainset = train_best_svd_model(ratings_df)\n",
    "    metrics = evaluate_model(algo, test_df, trainset, top_k=10,\n",
    "                             bewertung_df=bewertung_df,\n",
    "                             itemprofile_df=itemprofile_df)\n",
    "    print(\"Evaluation Results:\", metrics)\n",
    "\n",
    "    example_user = 243 # TODO: Hier kann die Nutzer-ID für einen beliebigen Nutzer gesetzt werden\n",
    "    recommendations = get_top_n_recommendations(\n",
    "        algo, trainset, example_user, 10, bewertung_df, itemprofile_df\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTop 10 recommendations for user {example_user}:\")\n",
    "    for item_id, score, confidence, explanation in recommendations:\n",
    "        print(f\"Item ID: {item_id}, Score: {score:.2f}, Confidence: {confidence}\")\n",
    "        print(f\"   → Explain: {explanation}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bewertung & Beispielausgabe**\n",
    "\n",
    "Die Evaluierung des SVD-Modells zeigt solide RMSE- und MAE-Werte, allerdings fällt die Precision@K mit 0.0035 und die Recall@K mit 0.0214 eher schwach aus. Das liegt vermutlich an der hohen Item-Vielfalt und sparsamen Nutzer-Item-Matrix, wodurch relevante Empfehlungen schwerer zu treffen sind.\n",
    "\n",
    "Als Beispiel zeigt die Ausgabe für Nutzer 243 zehn Top-Empfehlungen mit hoher Konfidenz. Neben der reinen Score-Normalisierung (0–1) wird jede Empfehlung durch eine einfache \"Explainability\"-Komponente ergänzt: Die finale Vorhersage ergibt sich aus dem globalen Durchschnitt, dem Nutzer-Bias, dem Item-Bias sowie der latenten Interaktion zwischen Nutzer- und Item-Vektor. Dadurch lassen sich Vorhersagen transparenter nachvollziehen, z. B. ob sie eher durch starke Item-Beliebtheit oder durch ein hohes Matching-Profil zwischen Nutzer und Item beeinflusst sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
